{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Домашнее задание 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 1. Количество твитов в наборе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Для начала мы подготовим наш файлы JSON к работе в Питоне и импортируем большинство необходимых нам модулей: для работы с JSON, счетчиками и регулярными выражениями.*\n",
    "\n",
    "**САМО РЕШЕНИЕ ЗАДАНИЯ.**\n",
    "\n",
    "Создаем нулевой счетчик *counter_all*. Одна строка в нашем файле отвечает за один твит, поэтому просто посчитаем строки: для каждой строки с помощью цикла for будем прибавлять к нашему изначально нулевому счетчику единичку.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import collections\n",
    "import re\n",
    "twitter = []\n",
    "counter_all = 0\n",
    "for line in open('hw_3_twitter.json', 'r', encoding='utf-8'):\n",
    "    twitter.append(json.loads(line))\n",
    "for line in twitter:\n",
    "    counter_all += 1\n",
    "print(counter_all)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 2. Количество удаленных твитов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводим нулевой счетчик *counter_del*. Затем с помощью цикла *for* для каждого твита ищем тег *delete*, которое встречается только в удаленных твитах. Если этот тег найдем, прибавляем к нашему счетчику единичку. Затем выходим из цикла, делим удаленные твиты на общее количество и умножаем результат на сто, чтобы найти процент удаленных твитов. Выводим результат _answer_ с помощью *print*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.162754303599373\n"
     ]
    }
   ],
   "source": [
    "counter_del = 0\n",
    "for line in twitter:\n",
    "    if \"delete\" in line:\n",
    "        counter_del += 1\n",
    "answer = (counter_del / counter_all) * 100\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 3. Самые популярные языки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводим нулевой счетчик _numb_ и пустой список *list_lang*. Затем проверяем каждый твит на наличие в нем тега языка (в удаленных твитах его не будет). Из твитов, в которых мы нашли этот тег, вынимаем значение ключа *lang* и добавляем это значение в пустой список. В итоге получаем список *list_lang* со всеми языками твитов, упомянутыми столько раз, сколько на них был написан твит. С помощью счетчика *Counter* из модуля *collections* создаем частотный словарь языков. Меняем местами ключи и значения (получается обратный словарь *new_dicti*), сортируем его по значениям в обратоном порядке (от большего к меньшему). Затем с помощью счетчика *numb* и цикла *for* распечатываем первые 10 пар значение - ключ. Готово! \n",
    "\n",
    "*(Потом я нашла более удобный способ сортировать словари по значениям, но в этом задании решила оставить так, как делала сначала, чтобы показать развитие моих программистских навыков)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Топ-10: \n",
      "\n",
      "en: 719\n",
      "ja: 438\n",
      "es: 173\n",
      "ko: 149\n",
      "th: 123\n",
      "ar: 119\n",
      "und: 117\n",
      "in: 71\n",
      "pt: 69\n",
      "fr: 35\n"
     ]
    }
   ],
   "source": [
    "list_lang = []\n",
    "numb = 0\n",
    "for line in twitter:\n",
    "    if \"lang\" in line:\n",
    "        list_lang.append(line['lang'])\n",
    "dicti = collections.Counter(list_lang)\n",
    "dicti = dict(dicti)\n",
    "new_dicti = dict(reversed(item) for item in dicti.items())\n",
    "new_dicti = collections.OrderedDict(sorted(new_dicti.items(), reverse = True))\n",
    "print('Топ-10:', '\\n')\n",
    "for key in new_dicti:\n",
    "    if numb < 10:\n",
    "        print(str(new_dicti[key])+':', str(key))\n",
    "    numb += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 4. Пользователи, написавшие более одного твита."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводим нулевой счетчик _numb_ и пустой список *list_user*. Затем проверяем каждый твит на наличие в нем тега *user* (в удаленных твитах его не будет). Из твитов, в которых мы нашли этот тег, вынимаем значение ключа *user*. Обозначаем значение этого ключа как *b* и ищем внутри него тег *id*. Вынимаем из него его значение и добавляем его в наш список *list_user*. В итоге получаем список *list_user* со всеми именами пользователей, упомянутыми столько раз, сколько ими было написано твитов. С помощью счетчика *Counter* из модуля *collections* создаем частотный словарь пользователей. Используя цикл *for*, смотрим на значение каждого ключа и, если оно больше одного (а значит, пользователь-ключ написал больше одного твита), добавляем к нашему счетчику единичку. В итоге распечатываем значение счетчика. Готово!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    }
   ],
   "source": [
    "list_user = []\n",
    "numb = 0\n",
    "for line in twitter:\n",
    "    if \"user\" in line:\n",
    "        b = line['user']\n",
    "        if \"id\" in b:\n",
    "            list_user.append(b[\"id\"])\n",
    "dicti = collections.Counter(list_user)\n",
    "dicti = dict(dicti)\n",
    "for key in dicti:\n",
    "    if int(dicti[key]) > 1:\n",
    "        numb += 1\n",
    "print(numb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 5. Топ-20 хэштегов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводим нулевой счетчик _numb_hash_ и пустой список *list_hash*. Вынимаем из твитов значение ключа _entities_. Обозначаем значение этого ключа как *b* и вынимаем из него значение ключа *hashtags*, обозначаем его как *c*. Избавляемся от пустых строк (получившихся из твитов без хэштега), задав условие, что *c* не является пустой строкой. Из оставшихся *c* вынимаем значение ключа *text* и добавляем его в наш список *list_hash*. В итоге получаем список *list_hash* со всеми хэштгами, упомянутыми столько раз, сколько они встретились в твитах. С помощью счетчика *Counter* из модуля *collections* создаем частотный словарь хэштегов. Затем используем метод *most_common* для получения 20 наиболее часто встречающихся тегов в порядке убывания. Распечатываем эти 20 хэштегов и количество твитов, в которых они встретились (то есть ключ и его значение из словаря. Готово!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-20 HASHTAGS: \n",
      "\n",
      "BTS: 17 twits\n",
      "방탄소년단: 13 twits\n",
      "AMAs: 11 twits\n",
      "人気投票ガチャ: 8 twits\n",
      "태형: 7 twits\n",
      "뷔: 6 twits\n",
      "BTSinChicago: 5 twits\n",
      "BTSLoveYourselfTour: 5 twits\n",
      "오늘의방탄: 5 twits\n",
      "PledgeForSwachhBharat: 5 twits\n",
      "MPN: 5 twits\n",
      "PCAs: 4 twits\n",
      "V: 4 twits\n",
      "시카고1회차공연: 4 twits\n",
      "เป๊กผลิตโชค: 4 twits\n",
      "JIMIN: 4 twits\n",
      "running: 3 twits\n",
      "NCT: 3 twits\n",
      "지민: 3 twits\n",
      "WajahmuPlastik: 3 twits\n"
     ]
    }
   ],
   "source": [
    "list_hash = []\n",
    "numb_hash = 0\n",
    "for line in twitter:\n",
    "    if \"entities\" in line:\n",
    "        b = line['entities']\n",
    "        if \"hashtags\" in b:\n",
    "            c = b[\"hashtags\"]\n",
    "            if c != []:\n",
    "                for line in c:\n",
    "                    list_hash.append(line['text'])\n",
    "dicti_hash = collections.Counter(list_hash)\n",
    "new_dicti_hash = dicti_hash.most_common(20)\n",
    "print('TOP-20 HASHTAGS:', '\\n')\n",
    "for one in new_dicti_hash:\n",
    "    print(str(one[0]) + ':', one[1], 'twits')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 6. Частотный словарь слов из твитов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вводим пустые списки *new_list_words* и *list_words*. Проверяем все твиты на следующие условия: не ретвит (нет *retweeted_status* в строке), не цитирование (нет *quoted_status* в строке), написан на английском языке (значение ключа *lang* -- *en*), в строке присутствует текст твита (*text*). Извлекаем текст твита и обозначаем его как *b*. Избавляемся от всех ссылок с помощью регулярного выражения. Избавляемся от всех знаков препинания с помощью регулярного выражения (апостроф не трогаем). Преобразуем все заглавные буквы в строчные. Разделяем каждый твит на слова и добавляем эти слова во второй список. Выходим из цикла. В итоге у нас получается список списков, и чтобы от этого избавиться, мы вынимаем из общего списка твитов каждый твит, а из каждого твита каждое слово, и по отдельности добавляем их во второй список. Теперь из этого списка составляем частотный словарь методом *Counter* и методом *most_common* извлекаем из него 50 самых частотных слов. Вводим пустой счетчик *count_word* и с его помощью распечатываем эти 50 самых частотных слов, как мы уже делали в предыдущем задании, но теперь и с их местами в топе. Готово!\n",
    "\n",
    "Это задание мы с Ксенией Петуховой и Романом Казаковым мы делали втроем."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-50 WORDS\n",
      "1. the: 106\n",
      "2. to: 78\n",
      "3. i: 71\n",
      "4. a: 69\n",
      "5. and: 58\n",
      "6. you: 47\n",
      "7. it: 43\n",
      "8. is: 41\n",
      "9. for: 40\n",
      "10. of: 40\n",
      "11. that: 34\n",
      "12. in: 32\n",
      "13. me: 26\n",
      "14. on: 25\n",
      "15. my: 25\n",
      "16. be: 22\n",
      "17. are: 20\n",
      "18. this: 19\n",
      "19. have: 19\n",
      "20. t: 18\n",
      "21. so: 18\n",
      "22. but: 17\n",
      "23. at: 17\n",
      "24. your: 16\n",
      "25. get: 16\n",
      "26. not: 16\n",
      "27. more: 16\n",
      "28. with: 16\n",
      "29. what: 15\n",
      "30. s: 15\n",
      "31. can: 14\n",
      "32. just: 14\n",
      "33. about: 13\n",
      "34. we: 12\n",
      "35. all: 11\n",
      "36. now: 11\n",
      "37. hit: 11\n",
      "38. up: 10\n",
      "39. 0: 10\n",
      "40. only: 10\n",
      "41. c: 10\n",
      "42. was: 10\n",
      "43. he: 10\n",
      "44. they: 10\n",
      "45. 6: 9\n",
      "46. out: 9\n",
      "47. 2: 9\n",
      "48. today: 9\n",
      "49. m: 9\n",
      "50. one: 9\n"
     ]
    }
   ],
   "source": [
    "list_words = []\n",
    "new_list_words = []\n",
    "for line in twitter:\n",
    "    if \"retweeted_status\" not in line:\n",
    "        if \"quoted_status\" not in line:\n",
    "            if \"lang\" in line:\n",
    "                if line[\"lang\"] == \"en\":\n",
    "                    if \"text\" in line:\n",
    "                        b = line['text']\n",
    "                        pattern = 'https://.+'\n",
    "                        b = re.sub(pattern, ' ', b)\n",
    "                        b = re.sub(r\"[^\\w^\\s\\']\", ' ', b)\n",
    "                        b = b.lower()\n",
    "                        b = b.split()\n",
    "                        list_words.append(b)\n",
    "for line in list_words:\n",
    "    for word in line:\n",
    "        new_list_words.append(word)\n",
    "dict_words = collections.Counter(new_list_words)\n",
    "new_dict_words = dict_words.most_common(50)\n",
    "print('TOP-50 WORDS')\n",
    "count_words = 1\n",
    "for word in new_dict_words:\n",
    "    if count_words <= 50:\n",
    "        print(str(count_words) + '.', str(word[0]) + ':', str(word[1]))\n",
    "        count_words += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 7. Топ по количеству фолловеров."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем два пустых списка и один счетчик. В каждом твите ищем тег _user_ и извлекаем значение этого ключа, обозначая его как *b*. Теперь внутри этого тега ищем тег *followers_count*, число подписчиков и *name*, имя пользователя. Добавляем значения этих двух ключей в первый список. Делаем из него кортеж (в нем будет два значения -- имя пользователя и число его подписчиков). Добавляем этот кортеж в большой список. В конце каждой итерации цикла обновляем первый список, снова обозначая его как пустой. После завершения итераций цикла превращаем второй, большой список в словарь. Сортируем его по значениям ключей в обратном порядке -- от большего к меньшему. С помощью счетчика, как и в прошлых заданиях, распечатываем первые 10 ключей и значений отсортированного словаря. Готово!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-10 USERS: \n",
      "\n",
      "1. Filosofía♕: 2521403\n",
      "2. FITNESS Magazine: 1491309\n",
      "3. malaysiakini.com: 1206759\n",
      "4. NYT Science: 1137374\n",
      "5. Gramática: 625463\n",
      "6. TGRT Haber: 392472\n",
      "7. The Sun Football ⚽: 383698\n",
      "8. Melbourne, Australia: 374222\n",
      "9. Roznama Express: 318189\n",
      "10. 💞 ცųཞɠɛཞცơơɠıɛ 💞: 311319\n"
     ]
    }
   ],
   "source": [
    "list_follower = []\n",
    "big_list = []\n",
    "count_foll = 1\n",
    "for line in twitter:\n",
    "    if \"user\" in line:\n",
    "        b = line['user']\n",
    "        if \"followers_count\" in b:\n",
    "            list_follower.append(b[\"name\"])\n",
    "            list_follower.append(b[\"followers_count\"])\n",
    "            list_follower = tuple(list_follower)\n",
    "            big_list.append(list_follower)\n",
    "            list_follower = []\n",
    "big_list = dict(big_list)\n",
    "new_big_list = sorted(big_list.items(), key=lambda x:x[1], reverse = True)\n",
    "print('TOP-10 USERS:', '\\n')\n",
    "for item in new_big_list:\n",
    "    if count_foll <= 10:\n",
    "        print(str(count_foll) + '.', str(item[0]) + ':', str(item[1]))\n",
    "        count_foll += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задание 8. Топ источники твитов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем два пустых списка. Ищем в каждом из твитов тег *source*. Извлекаем значения найденных ключей, обозначая получаемые данные как *source*. Внутри этих данных ищем непосредственно источник публикации (заглянув в наш файл через *TextWrangler*, я увидела, чем именно окружены источники, и на основании этого составила регулярное выражение, которое их извлечет) и извлекаем его, обозначив как *source1*. Добавляю найденный краткий источник в первый список. Снова, как и в задании 7, получился список списков. Опять извлекаем из него каждый элемент и добавляем в новый список, предварительно проверив, не является ли этот элемент пустой строкой. Из нового списка составляем частотный словарь. Из частотного словаря извлекаем 10 наиболее часто встречающихся ключей-источников вместе с их значениями с помощью *most_common*. Как и в предыдущих заданиях, распечатываем топ-10 с помощью счетчика, изначально равнявшегося единице. Готово!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOP-10 SOURCES:\n",
      "1. Twitter for iPhone: 800\n",
      "2. Twitter for Android: 695\n",
      "3. Twitter Web Client: 140\n",
      "4. twittbot.net: 122\n",
      "5. Twitter Lite: 51\n",
      "6. Twitter for iPad: 28\n",
      "7. TweetDeck: 23\n",
      "8. Facebook: 17\n",
      "9. IFTTT: 14\n",
      "10. تطبيق قرآني: 10\n"
     ]
    }
   ],
   "source": [
    "list_source = []\n",
    "new_list_source = []\n",
    "for line in twitter:\n",
    "    if \"source\" in line:\n",
    "        source = line['source']\n",
    "        source1 = re.findall(r'\\u003e(.+)\\u003c\\/a\\u003e', source)\n",
    "        list_source.append(source1)\n",
    "for item in list_source:\n",
    "    for thing in item:\n",
    "        if thing != []:\n",
    "            new_list_source.append(thing)\n",
    "counter_list_source = collections.Counter(new_list_source)\n",
    "print('TOP-10 SOURCES:')\n",
    "count_words = 1\n",
    "new_counter_list_source = counter_list_source.most_common(10)\n",
    "for word in new_counter_list_source:\n",
    "    if count_words <= 10:\n",
    "        print(str(count_words) + '.', str(word[0]) + ':', str(word[1]))\n",
    "        count_words += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
